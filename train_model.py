import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import os
from sklearn.utils import class_weight
import numpy as np

# ==========================================
# 1. C·∫§U H√åNH T·ªêI ∆ØU (HYPERPARAMETERS)
# ==========================================
DATA_DIR = 'dataset'
IMG_SIZE = (300, 300)  # EfficientNetB3 t·ªëi ∆∞u ·ªü k√≠ch th∆∞·ªõc 300x300
BATCH_SIZE = 16        # Gi·∫£m batch size ƒë·ªÉ v·ª´a VRAM 8GB (v√¨ ·∫£nh to h∆°n)
EPOCHS_HEAD = 10       # S·ªë epoch train kh·ªüi ƒë·ªông
EPOCHS_FINE = 50       # S·ªë epoch train tinh ch·ªânh (s·∫Ω d·ª´ng s·ªõm n·∫øu c·∫ßn)

# Ki·ªÉm tra GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"‚úÖ ƒêang ch·∫°y tr√™n GPU: {gpus[0]}")
    except RuntimeError as e:
        print(e)

# ==========================================
# 2. LOAD DATASET CHU·∫®N
# ==========================================
print("\n--- ƒêang t·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu ---")

train_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int' # D√πng sparse categorical crossentropy cho ti·∫øt ki·ªám RAM
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_DIR,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    label_mode='int'
)

class_names = train_ds.class_names
num_classes = len(class_names)
print(f"Classes: {class_names}")
# ==========================================
# 2.5. T√çNH TO√ÅN CLASS WEIGHTS (FIX L·ªñI JSON)
# ==========================================
print("\n--- ƒêang t√≠nh to√°n Class Weights ---")

# L·∫•y nh√£n t·ª´ t·∫≠p train ƒë·ªÉ t√≠nh to√°n
# L∆∞u √Ω: train_ds l√† d·∫°ng Batch, c·∫ßn n·ªëi l·∫°i
train_labels = []
for images, labels in train_ds.unbatch():
    train_labels.append(labels.numpy())

train_labels = np.array(train_labels)

# T√≠nh to√°n tr·ªçng s·ªë
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels),
    y=train_labels
)

# QUAN TR·ªåNG: Chuy·ªÉn v·ªÅ float thu·∫ßn c·ªßa Python ƒë·ªÉ tr√°nh l·ªói JSON Serialized
class_weights_dict = {i : float(w) for i, w in enumerate(class_weights)}

print("Class Weights (ƒê√£ fix l·ªói):")
print(class_weights_dict)

# T·ªëi ∆∞u hi·ªáu nƒÉng pipeline
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# ==========================================
# 3. DATA AUGMENTATION N√ÇNG CAO
# ==========================================
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),       # Xoay t·ªëi ƒëa 20%
    layers.RandomZoom(0.2),           # Zoom
    layers.RandomContrast(0.2),       # Thay ƒë·ªïi t∆∞∆°ng ph·∫£n (quan tr·ªçng cho da li·ªÖu)
    layers.RandomBrightness(0.2),     # Thay ƒë·ªïi ƒë·ªô s√°ng
])

# ==========================================
# 4. X√ÇY D·ª∞NG MODEL (EFFICIENTNET B3)
# ==========================================
print("\n--- Kh·ªüi t·∫°o EfficientNetB3 ---")

# T·∫£i base model
base_model = tf.keras.applications.EfficientNetB3(
    input_shape=IMG_SIZE + (3,),
    include_top=False,
    weights='imagenet'
)

# Ban ƒë·∫ßu ƒë√≥ng bƒÉng to√†n b·ªô base
base_model.trainable = False

inputs = layers.Input(shape=IMG_SIZE + (3,))
x = data_augmentation(inputs)

# EfficientNet c√≥ s·∫µn l·ªõp x·ª≠ l√Ω input, nh∆∞ng ta d√πng preprocess_input cho ch·∫Øc ch·∫Øn n·∫øu c·∫ßn
# x = tf.keras.applications.efficientnet.preprocess_input(x)

x = base_model(x, training=False) # training=False ƒë·ªÉ gi·ªØ nguy√™n BatchNormalization
x = layers.GlobalAveragePooling2D()(x)
x = layers.BatchNormalization()(x) # Gi√∫p ·ªïn ƒë·ªãnh training
x = layers.Dropout(0.3)(x)         # TƒÉng dropout l√™n 0.3 ƒë·ªÉ ch·ªëng overfitting
outputs = layers.Dense(num_classes, activation='softmax')(x)

model = models.Model(inputs, outputs)

# ==========================================
# 5. C√ÅC CALLBACKS QUAN TR·ªåNG
# ==========================================
# L∆∞u model t·ªët nh·∫•t (kh√¥ng ph·∫£i model cu·ªëi c√πng)
checkpoint = ModelCheckpoint(
    "best_skin_model_v2.h5",
    monitor='val_accuracy',
    save_best_only=True,
    save_weights_only=True,
    mode='max',
    verbose=1
)

# D·ª´ng train n·∫øu kh√¥ng ti·∫øn b·ªô sau 7 epoch
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=7,
    restore_best_weights=True,
    verbose=1
)

# Gi·∫£m Learning Rate n·∫øu Loss ƒëi ngang (gi√∫p h·ªôi t·ª• s√¢u h∆°n)
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,    # Gi·∫£m 5 l·∫ßn (nh√¢n 0.2)
    patience=3,    # Sau 3 epoch kh√¥ng kh√° h∆°n th√¨ gi·∫£m
    min_lr=1e-6,
    verbose=1
)

# ==========================================
# 6. GIAI ƒêO·∫†N 1: WARM-UP (TRAIN HEAD)
# ==========================================
print("\nüî• GIAI ƒêO·∫†N 1: Train l·ªõp Classifier (Warm-up)...")
model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-3), # LR ban ƒë·∫ßu l·ªõn
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS_HEAD,
    callbacks=[checkpoint] # Ch·ªâ l∆∞u checkpoint, ch∆∞a c·∫ßn gi·∫£m LR
)

# ==========================================
# 7. GIAI ƒêO·∫†N 2: FINE-TUNING TO√ÄN B·ªò
# ==========================================
print("\nüî•üî• GIAI ƒêO·∫†N 2: Unfreeze to√†n b·ªô v√† Train s√¢u...")

# M·ªü kh√≥a to√†n b·ªô model ƒë·ªÉ h·ªçc c√°c ƒë·∫∑c tr∆∞ng chi ti·∫øt c·ªßa da
base_model.trainable = True

# Quan tr·ªçng: Khi fine-tune ph·∫£i d√πng Learning Rate R·∫§T NH·ªé
# N·∫øu kh√¥ng s·∫Ω ph√° h·ªèng c√°c tr·ªçng s·ªë ƒë√£ h·ªçc ·ªü ImageNet
model.compile(
    optimizer=optimizers.Adam(learning_rate=1e-4), # Nh·ªè h∆°n 10 l·∫ßn
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# N·ªëi ti·∫øp history
total_epochs = EPOCHS_HEAD + EPOCHS_FINE

history_2 = model.fit(
    train_ds,
    validation_data=val_ds,
    initial_epoch=history_1.epoch[-1],
    epochs=total_epochs,
    callbacks=[checkpoint, early_stopping, lr_scheduler], # Th√™m ƒë·∫ßy ƒë·ªß "v≈© kh√≠"
    class_weight=class_weights_dict
)

# ==========================================
# 8. V·∫º BI·ªÇU ƒê·ªí B√ÅO C√ÅO
# ==========================================
acc = history_1.history['accuracy'] + history_2.history['accuracy']
val_acc = history_1.history['val_accuracy'] + history_2.history['val_accuracy']
loss = history_1.history['loss'] + history_2.history['loss']
val_loss = history_1.history['val_loss'] + history_2.history['val_loss']

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
# V·∫Ω ƒë∆∞·ªùng ngƒÉn c√°ch 2 giai ƒëo·∫°n
plt.axvline(x=EPOCHS_HEAD-1, color='green', linestyle='--', label='Start Fine-Tuning')
plt.legend(loc='lower right')
plt.title('Training Accuracy (EfficientNetB3)')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.axvline(x=EPOCHS_HEAD-1, color='green', linestyle='--')
plt.legend(loc='upper right')
plt.title('Training Loss')

plt.savefig('training_result_optimized.png')
print("\n‚úÖ ƒê√£ ho√†n t·∫•t! Model l∆∞u t·∫°i: best_skin_model_v2.keras")
print("Bi·ªÉu ƒë·ªì k·∫øt qu·∫£: training_result_optimized.png")

# L∆∞u l·∫°i class names
with open('class_names.txt', 'w') as f:
    for cls in class_names:
        f.write(f"{cls}\n")